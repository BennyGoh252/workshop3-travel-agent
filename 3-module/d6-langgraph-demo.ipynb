{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# LangGraph Demo\n",
    "\n",
    "This notebook demonstrates **LangGraph**, the most sophisticated framework for building stateful, multi-agent applications with complex workflows using graph-based architectures.\n",
    "\n",
    "## Required Environment Variables\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY # Your OpenAI API key (required)\n",
    "```\n",
    "\n",
    "Please refer to the [README](README.md) for instructions on setting up environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dyfmn5ox4n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running standalone without the project setup:\n",
    "# pip install langgraph openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from typing import Dict, List, Literal, TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Import LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# For visualization\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3kcxut4ul7i",
   "metadata": {},
   "source": [
    "### LangGraph Core Components Loaded:\n",
    "- **StateGraph:** Graph-based workflow orchestrator\n",
    "- **State Management:** TypedDict-based state schemas\n",
    "- **Conditional Logic:** Dynamic routing capabilities\n",
    "- **LLM Integration:** ChatOpenAI for intelligent node processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-concept",
   "metadata": {},
   "source": [
    "## State Management\n",
    "\n",
    "In LangGraph, **State** is the cornerstone that makes complex workflows possible. Unlike other frameworks where data flows through function calls, LangGraph maintains a **persistent, shared state** that evolves through the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our state schema using TypedDict\n",
    "class TicketState(TypedDict):\n",
    "    \"\"\"State schema for our customer support ticket routing system\"\"\"\n",
    "    # Ticket information\n",
    "    ticket_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    customer_tier: str\n",
    "\n",
    "    # Processing state\n",
    "    category: str\n",
    "    priority: str\n",
    "    assigned_agent: str\n",
    "\n",
    "    # Workflow tracking\n",
    "    processing_steps: Annotated[List[str], lambda x, y: x + y]  # List reducer\n",
    "    status: str\n",
    "\n",
    "    # Results\n",
    "    resolution: str\n",
    "\n",
    "# Example initial state\n",
    "sample_state = TicketState(\n",
    "    ticket_id=\"TKT-12345\",\n",
    "    title=\"Login Issues with Mobile App\",\n",
    "    description=\"Customer unable to log in using mobile app, web works fine\",\n",
    "    customer_tier=\"Premium\",\n",
    "    category=\"\",\n",
    "    priority=\"\",\n",
    "    assigned_agent=\"\",\n",
    "    processing_steps=[],\n",
    "    status=\"New\",\n",
    "    resolution=\"\"\n",
    ")\n",
    "\n",
    "print(sample_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3up574omlus",
   "metadata": {},
   "source": [
    "### State Schema Defined: TicketState\n",
    "\n",
    "**State Fields:**\n",
    "- **Ticket Info:** ticket_id, title, description, customer_tier\n",
    "- ️ **Processing:** category, priority, assigned_agent\n",
    "- **Workflow:** processing_steps (with list reducer), status\n",
    "- **Results:** resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtswda27mbh",
   "metadata": {},
   "source": [
    "## Visualizing the AI Workflow as a Graph\n",
    "\n",
    "Before we demonstrate the workflow execution, let's visualize how LangGraph structures the workflow as a graph. This shows how the state will flow through each AI-powered decision node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dk9lcgleghv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM for AI-powered workflow\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Node 1: Ticket Classifier with LLM\n",
    "def classify_ticket(state: TicketState) -> dict:\n",
    "    \"\"\"Classify ticket into category using LLM analysis\"\"\"\n",
    "\n",
    "    print(f\"Classifying ticket: {state['ticket_id']}\")\n",
    "\n",
    "    # Use LLM to classify the ticket\n",
    "    classification_prompt = f\"\"\"\n",
    "    Analyze this support ticket and classify it into one of these categories:\n",
    "    - Technical: Login issues, bugs, errors, system problems\n",
    "    - Billing: Payment, subscription, pricing issues\n",
    "    - General: All other inquiries\n",
    "\n",
    "    Ticket Title: {state['title']}\n",
    "    Ticket Description: {state['description']}\n",
    "\n",
    "    Respond with ONLY the category name (Technical, Billing, or General).\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a ticket classification expert.\"),\n",
    "        HumanMessage(content=classification_prompt)\n",
    "    ])\n",
    "\n",
    "    category = response.content.strip()\n",
    "\n",
    "    # Validate and default to General if unexpected response\n",
    "    if category not in [\"Technical\", \"Billing\", \"General\"]:\n",
    "        category = \"General\"\n",
    "\n",
    "    # Return state update (partial update, not full state)\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"processing_steps\": [f\"Classified as {category} by AI analysis\"],\n",
    "        \"status\": \"Classified\"\n",
    "    }\n",
    "\n",
    "# Node 2: Priority Router with LLM\n",
    "def route_priority(state: TicketState) -> dict:\n",
    "    \"\"\"Determine ticket priority using LLM reasoning\"\"\"\n",
    "\n",
    "    print(f\"Routing priority for {state['category']} ticket from {state['customer_tier']} customer\")\n",
    "\n",
    "    # Use LLM to determine priority\n",
    "    priority_prompt = f\"\"\"\n",
    "    Determine the priority level for this support ticket based on:\n",
    "    - Category: {state['category']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "\n",
    "    Priority Rules:\n",
    "    - Premium customers with Technical issues should be High priority\n",
    "    - Premium customers with other issues should be Medium priority\n",
    "    - Standard customers with Technical issues should be Medium priority\n",
    "    - Standard customers with other issues should be Low priority\n",
    "    - Critical system outages should always be High priority\n",
    "\n",
    "    Respond with ONLY the priority level (High, Medium, or Low).\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support ticket priority expert.\"),\n",
    "        HumanMessage(content=priority_prompt)\n",
    "    ])\n",
    "\n",
    "    priority = response.content.strip()\n",
    "\n",
    "    # Validate and set default\n",
    "    if priority not in [\"High\", \"Medium\", \"Low\"]:\n",
    "        priority = \"Medium\"\n",
    "\n",
    "    step_msg = f\"Priority set to {priority} by AI assessment ({state['customer_tier']} customer, {state['category']} issue)\"\n",
    "\n",
    "    return {\n",
    "        \"priority\": priority,\n",
    "        \"processing_steps\": [step_msg],\n",
    "        \"status\": \"Prioritized\"\n",
    "    }\n",
    "\n",
    "# Node 3: Agent Matcher with LLM\n",
    "def match_agent(state: TicketState) -> dict:\n",
    "    \"\"\"Assign appropriate agent using LLM matching\"\"\"\n",
    "\n",
    "    print(f\"Matching agent for {state['priority']} priority {state['category']} ticket\")\n",
    "\n",
    "    # Use LLM to match the best agent\n",
    "    agent_prompt = f\"\"\"\n",
    "    Match the best support agent for this ticket:\n",
    "    - Category: {state['category']}\n",
    "    - Priority: {state['priority']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "    - Issue: {state['title']}\n",
    "\n",
    "    Available agents:\n",
    "    - Senior Tech Support: For high-priority technical issues\n",
    "    - Tech Support: For standard technical issues\n",
    "    - Billing Specialist: For all billing-related issues\n",
    "    - General Support: For general inquiries and low-priority issues\n",
    "\n",
    "    Respond with ONLY the agent role name.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are an expert at matching support tickets to the right agents.\"),\n",
    "        HumanMessage(content=agent_prompt)\n",
    "    ])\n",
    "\n",
    "    agent = response.content.strip()\n",
    "\n",
    "    # Validate agent assignment\n",
    "    valid_agents = [\"Senior Tech Support\", \"Tech Support\", \"Billing Specialist\", \"General Support\"]\n",
    "    if agent not in valid_agents:\n",
    "        # Fallback logic based on category\n",
    "        if state['category'] == \"Technical\":\n",
    "            agent = \"Tech Support\"\n",
    "        elif state['category'] == \"Billing\":\n",
    "            agent = \"Billing Specialist\"\n",
    "        else:\n",
    "            agent = \"General Support\"\n",
    "\n",
    "    step_msg = f\"Assigned to {agent} by AI matching based on {state['category']}/{state['priority']}\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": agent,\n",
    "        \"processing_steps\": [step_msg],\n",
    "        \"status\": \"Assigned\"\n",
    "    }\n",
    "\n",
    "# Node 4: Resolution Generator\n",
    "def generate_resolution(state: TicketState) -> dict:\n",
    "    \"\"\"Use LLM to generate a resolution for the ticket\"\"\"\n",
    "\n",
    "    resolution_prompt = f\"\"\"\n",
    "    Generate a brief, professional resolution message for this support ticket:\n",
    "\n",
    "    Ticket: {state['title']}\n",
    "    Category: {state['category']}\n",
    "    Priority: {state['priority']}\n",
    "    Assigned Agent: {state['assigned_agent']}\n",
    "    Customer Tier: {state['customer_tier']}\n",
    "\n",
    "    Create a resolution message (max 30 words) that confirms the issue was addressed.\n",
    "    Be specific to the issue type and professional.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support resolution specialist.\"),\n",
    "        HumanMessage(content=resolution_prompt)\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"resolution\": response.content.strip(),\n",
    "        \"processing_steps\": state[\"processing_steps\"] + [\"Resolution generated by AI\"],\n",
    "        \"status\": \"Resolved\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0n8h7adq8wz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the demonstration workflow as a LangGraph\n",
    "def create_demo_workflow():\n",
    "    \"\"\"Build the AI demonstration workflow as a graph\"\"\"\n",
    "\n",
    "    # Initialize the StateGraph\n",
    "    workflow = StateGraph(TicketState)\n",
    "\n",
    "    # Add the 4 nodes from our demonstration\n",
    "    workflow.add_node(\"classify\", classify_ticket)\n",
    "    workflow.add_node(\"prioritize\", route_priority)\n",
    "    workflow.add_node(\"assign\", match_agent)\n",
    "    workflow.add_node(\"resolve\", generate_resolution)\n",
    "\n",
    "    # Add sequential edges connecting all nodes\n",
    "    workflow.add_edge(\"classify\", \"prioritize\")\n",
    "    workflow.add_edge(\"prioritize\", \"assign\")\n",
    "    workflow.add_edge(\"assign\", \"resolve\")\n",
    "    workflow.add_edge(\"resolve\", END)\n",
    "\n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"classify\")\n",
    "\n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "# Create and visualize the demo workflow\n",
    "demo_app = create_demo_workflow()\n",
    "\n",
    "print(\"🔍 AI Workflow Graph - The Path We're About to Execute:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Display Mermaid diagram (renders automatically in Jupyter)\n",
    "from IPython.display import Markdown\n",
    "\n",
    "try:\n",
    "    mermaid_text = demo_app.get_graph().draw_mermaid()\n",
    "    # Clean up the mermaid text - remove the config section for cleaner output\n",
    "    lines = mermaid_text.split('\\n')\n",
    "    # Skip the config section if present\n",
    "    start_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'graph TD' in line or 'graph LR' in line:\n",
    "            start_idx = i\n",
    "            break\n",
    "    clean_mermaid = '\\n'.join(lines[start_idx:])\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "### 📊 Graph Structure (Interactive Mermaid Diagram):\n",
    "\n",
    "```mermaid\n",
    "{clean_mermaid}\n",
    "```\n",
    "\"\"\"))\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate Mermaid diagram: {e}\")\n",
    "\n",
    "# Also show ASCII as text fallback\n",
    "print(\"\\n📊 Graph Structure (ASCII):\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    ascii_graph = demo_app.get_graph().draw_ascii()\n",
    "    print(ascii_graph)\n",
    "except Exception as e:\n",
    "    print(f\"ASCII visualization not available: {e}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n📝 Graph Explanation:\")\n",
    "print(\"  • Each box represents an AI-powered decision node\")\n",
    "print(\"  • Arrows show the flow of state through the workflow\")\n",
    "print(\"  • The state accumulates information at each step\")\n",
    "print(\"  • Every node uses LLM intelligence for decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1serg5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's execute the workflow using the graph we just visualized\n",
    "def execute_graph_workflow():\n",
    "    \"\"\"Execute the workflow through the LangGraph we created\"\"\"\n",
    "\n",
    "    print(\"🚀 Executing the Workflow Through LangGraph\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Prepare a test ticket\n",
    "    test_ticket = {\n",
    "        \"ticket_id\": \"TKT-GRAPH-001\",\n",
    "        \"title\": \"Mobile app crashes on startup\",\n",
    "        \"description\": \"The iOS app crashes immediately after opening. This is affecting my productivity.\",\n",
    "        \"customer_tier\": \"Premium\",\n",
    "        \"category\": \"\",\n",
    "        \"priority\": \"\",\n",
    "        \"assigned_agent\": \"\",\n",
    "        \"processing_steps\": [],\n",
    "        \"status\": \"New\",\n",
    "        \"resolution\": \"\"\n",
    "    }\n",
    "\n",
    "    print(\"\\n📥 Input Ticket:\")\n",
    "    print(f\"  ID: {test_ticket['ticket_id']}\")\n",
    "    print(f\"  Title: {test_ticket['title']}\")\n",
    "    print(f\"  Customer: {test_ticket['customer_tier']} tier\")\n",
    "    print(f\"  Status: {test_ticket['status']}\")\n",
    "\n",
    "    print(\"\\n🔄 Processing through LangGraph...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    try:\n",
    "        # Execute the workflow using the graph\n",
    "        result = demo_app.invoke(test_ticket)\n",
    "\n",
    "        print(\"\\n✅ Graph Execution Complete!\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        print(\"\\n📊 Final State After Graph Processing:\")\n",
    "        print(f\"  Ticket ID: {result['ticket_id']}\")\n",
    "        print(f\"  Category: {result['category']}\")\n",
    "        print(f\"  Priority: {result['priority']}\")\n",
    "        print(f\"  Assigned Agent: {result['assigned_agent']}\")\n",
    "        print(f\"  Status: {result['status']}\")\n",
    "        print(f\"  Resolution: {result['resolution']}\")\n",
    "\n",
    "        print(\"\\n🔄 Execution Path Through Graph:\")\n",
    "        for i, step in enumerate(result['processing_steps'], 1):\n",
    "            print(f\"  {i}. {step}\")\n",
    "\n",
    "        print(\"\\n💡 Key Insights:\")\n",
    "        print(\"  • The graph orchestrated the entire workflow\")\n",
    "        print(\"  • Each node processed the state sequentially\")\n",
    "        print(\"  • State accumulated through the execution path\")\n",
    "        print(\"  • LLM decisions were made at each node\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Execution Error: {str(e)}\")\n",
    "        print(\"\\nThis might be due to:\")\n",
    "        print(\"  • Missing OpenAI API key\")\n",
    "        print(\"  • API rate limits\")\n",
    "        print(\"  • Network issues\")\n",
    "        return None\n",
    "\n",
    "# Execute the workflow through the graph\n",
    "print(\"Now let's run a ticket through the graph we just visualized:\\n\")\n",
    "graph_result = execute_graph_workflow()\n",
    "\n",
    "if graph_result:\n",
    "    print(\"\\n🎯 Graph-Based Workflow Success!\")\n",
    "    print(\"The LangGraph successfully processed the ticket through all nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nodes-edges-concept",
   "metadata": {},
   "source": [
    "## Nodes & Edges\n",
    "\n",
    "**Nodes** and **Edges** are the building blocks of LangGraph workflows. Together, they create sophisticated processing pipelines.\n",
    "\n",
    "### Nodes: State Processing Functions\n",
    "- **Input**: Current state\n",
    "- **Processing**: Business logic, LLM calls, API requests\n",
    "- **Output**: Updated state (partial or complete)\n",
    "- **Features**: Error handling, logging, side effects\n",
    "\n",
    "### Edges: Flow Control Mechanisms\n",
    "- **Fixed Edges**: Always go to the same next node\n",
    "- **Conditional Edges**: Dynamic routing based on state\n",
    "- **END Edge**: Terminate workflow\n",
    "\n",
    "### Node-to-Node Communication:\n",
    "Unlike function chaining, nodes communicate through **shared state**. Each node sees the complete workflow history and can make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-concept",
   "metadata": {},
   "source": [
    "## Conditional Edges & Dynamic Workflows\n",
    "\n",
    "**Conditional Edges** are where LangGraph truly shines. Unlike fixed linear flows, conditional edges enable **dynamic routing** based on state values, creating sophisticated branching workflows.\n",
    "\n",
    "### Conditional Edge Capabilities:\n",
    "- **Dynamic Routing**: Different paths based on state conditions\n",
    "- **Multi-Path Logic**: Handle success/error/retry scenarios\n",
    "- **State-Based Decisions**: Route based on any state field\n",
    "- **Complex Conditions**: Multiple criteria and nested logic\n",
    "- **Cycles & Loops**: Retry mechanisms and iterative processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-nodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced state for conditional workflows\n",
    "class EnhancedTicketState(TypedDict):\n",
    "    \"\"\"Enhanced state with conditional routing fields\"\"\"\n",
    "    # Basic ticket info\n",
    "    ticket_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    customer_tier: str\n",
    "\n",
    "    # Processing state\n",
    "    category: str\n",
    "    priority: str\n",
    "    assigned_agent: str\n",
    "\n",
    "    # Conditional routing fields\n",
    "    needs_escalation: bool\n",
    "\n",
    "    # Workflow tracking\n",
    "    processing_steps: Annotated[List[str], lambda x, y: x + y]\n",
    "    status: str\n",
    "    resolution: str\n",
    "\n",
    "# Initialize LLMs for different agent levels\n",
    "llm_junior = ChatOpenAI(model=\"gpt-4o-mini\")  # Normal agent\n",
    "llm_senior = ChatOpenAI(model=\"gpt-4\")  # Senior agent (more capable)\n",
    "\n",
    "print(\"🤖 LLM Agents Initialized:\")\n",
    "print(\"  • Junior Agent: GPT-4o-mini (efficient for standard issues)\")\n",
    "print(\"  • Senior Agent: GPT-4 (advanced reasoning for complex issues)\")\n",
    "\n",
    "# Node for escalation check\n",
    "def escalation_check(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Use LLM to determine if ticket needs escalation to senior agent\"\"\"\n",
    "\n",
    "    print(f\"Checking if ticket needs escalation...\")\n",
    "\n",
    "    # Use LLM for intelligent escalation decision\n",
    "    escalation_prompt = f\"\"\"\n",
    "    Analyze if this ticket needs escalation to a senior agent:\n",
    "\n",
    "    Ticket Details:\n",
    "    - Priority: {state['priority']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "    - Category: {state['category']}\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "\n",
    "    Escalation Criteria:\n",
    "    - High priority tickets should be escalated\n",
    "    - Premium customers with technical issues should be escalated\n",
    "    - Any mention of \"critical\", \"urgent\", \"outage\", \"production\", or \"down\" should be escalated\n",
    "    - Complex technical problems requiring deep expertise should be escalated\n",
    "\n",
    "    Respond with ONLY \"YES\" or \"NO\" for escalation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are an escalation specialist who determines if tickets need senior attention.\"),\n",
    "        HumanMessage(content=escalation_prompt)\n",
    "    ])\n",
    "\n",
    "    needs_escalation = response.content.strip().upper() == \"YES\"\n",
    "\n",
    "    step_msg = f\"Escalation decision: {'Route to Senior Agent (GPT-4)' if needs_escalation else 'Route to Normal Agent (GPT-4o-mini)'}\"\n",
    "\n",
    "    return {\n",
    "        \"needs_escalation\": needs_escalation,\n",
    "        \"processing_steps\": [step_msg],\n",
    "        \"status\": \"Escalation Checked\"\n",
    "    }\n",
    "\n",
    "# Senior Agent resolution (using GPT-4)\n",
    "def senior_agent_resolution(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Senior agent with GPT-4 handles complex escalated tickets\"\"\"\n",
    "\n",
    "    print(f\"🔴 SENIOR AGENT (GPT-4) handling escalated ticket\")\n",
    "\n",
    "    # Use more capable GPT-4 for complex problem solving\n",
    "    resolution_prompt = f\"\"\"\n",
    "    You are a SENIOR support agent with advanced expertise. This ticket has been escalated to you.\n",
    "\n",
    "    Ticket Details:\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "    - Category: {state['category']}\n",
    "    - Priority: {state['priority']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "\n",
    "    As a senior agent with GPT-4 capabilities, provide:\n",
    "    1. Deep technical analysis of the issue\n",
    "    2. Comprehensive solution with detailed steps\n",
    "    3. Preventive measures to avoid future occurrences\n",
    "\n",
    "    Create a detailed resolution (50-75 words) that demonstrates senior-level expertise.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_senior.invoke([\n",
    "        SystemMessage(content=\"You are a senior technical expert with deep knowledge and problem-solving capabilities.\"),\n",
    "        HumanMessage(content=resolution_prompt)\n",
    "    ])\n",
    "\n",
    "    resolution_msg = f\"[SENIOR AGENT - GPT-4] {response.content.strip()}\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": \"Senior Agent (GPT-4)\",\n",
    "        \"resolution\": resolution_msg,\n",
    "        \"processing_steps\": [\"Escalated ticket resolved by Senior Agent using GPT-4's advanced capabilities\"],\n",
    "        \"status\": \"Resolved by Senior Agent\"\n",
    "    }\n",
    "\n",
    "# Normal Agent resolution (using GPT-4o-mini)\n",
    "def normal_agent_resolution(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Normal agent with GPT-4o-mini handles standard tickets\"\"\"\n",
    "\n",
    "    print(f\"🟢 NORMAL AGENT (GPT-4o-mini) handling standard ticket\")\n",
    "\n",
    "    # Use efficient GPT-4o-mini for standard issues\n",
    "    resolution_prompt = f\"\"\"\n",
    "    You are a standard support agent handling routine tickets.\n",
    "\n",
    "    Ticket Details:\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "    - Category: {state['category']}\n",
    "    - Priority: {state['priority']}\n",
    "\n",
    "    Provide a helpful resolution (25-30 words) for this standard issue.\n",
    "    Be concise and practical.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_junior.invoke([\n",
    "        SystemMessage(content=\"You are a helpful support agent handling routine customer issues.\"),\n",
    "        HumanMessage(content=resolution_prompt)\n",
    "    ])\n",
    "\n",
    "    resolution_msg = f\"[NORMAL AGENT - GPT-4o-mini] {response.content.strip()}\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": \"Normal Agent (GPT-4o-mini)\",\n",
    "        \"resolution\": resolution_msg,\n",
    "        \"processing_steps\": [\"Standard ticket resolved by Normal Agent using GPT-4o-mini\"],\n",
    "        \"status\": \"Resolved by Normal Agent\"\n",
    "    }\n",
    "\n",
    "print(\"\\n📊 Conditional Workflow Architecture:\")\n",
    "print(\"  🔀 Escalation Check: AI determines ticket complexity\")\n",
    "print(\"  🔴 Escalated Path → Senior Agent (GPT-4)\")\n",
    "print(\"     • Handles complex technical issues\")\n",
    "print(\"     • Provides deep analysis and comprehensive solutions\")\n",
    "print(\"     • No approval needed - senior agents have authority\")\n",
    "print(\"  🟢 Normal Path → Normal Agent (GPT-4o-mini)\")\n",
    "print(\"     • Handles routine customer issues\")\n",
    "print(\"     • Provides quick, practical solutions\")\n",
    "print(\"     • Efficient processing for standard tickets\")\n",
    "\n",
    "print(\"\\n💡 Key Difference:\")\n",
    "print(\"  • Escalated tickets get MORE CAPABLE AI (GPT-4)\")\n",
    "print(\"  • Normal tickets get EFFICIENT AI (GPT-4o-mini)\")\n",
    "print(\"  • Different paths lead to DIFFERENT OUTCOMES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-routing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditional routing functions\n",
    "def route_after_escalation_check(state: EnhancedTicketState) -> Literal[\"senior_agent\", \"normal_agent\"]:\n",
    "    \"\"\"Route based on escalation needs\"\"\"\n",
    "    if state['needs_escalation']:\n",
    "        return \"senior_agent\"  # Goes to GPT-4 powered senior agent\n",
    "    else:\n",
    "        return \"normal_agent\"  # Goes to GPT-4o-mini powered normal agent\n",
    "\n",
    "print(\"🔀 Routing Logic Defined:\")\n",
    "print(\"  • needs_escalation = True → 'senior_agent' (GPT-4)\")\n",
    "print(\"  • needs_escalation = False → 'normal_agent' (GPT-4o-mini)\")\n",
    "print(\"\\n✨ This creates truly different paths with different AI capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-conditional-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the conditional workflow graph with different agent paths\n",
    "def create_conditional_workflow():\n",
    "    \"\"\"Create a workflow with truly different conditional paths\"\"\"\n",
    "\n",
    "    # Initialize the StateGraph with enhanced state\n",
    "    workflow = StateGraph(EnhancedTicketState)\n",
    "\n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"classifier\", classify_ticket)\n",
    "    workflow.add_node(\"priority_router\", route_priority)\n",
    "    workflow.add_node(\"escalation_check\", escalation_check)\n",
    "    workflow.add_node(\"senior_agent\", senior_agent_resolution)  # GPT-4 powered\n",
    "    workflow.add_node(\"normal_agent\", normal_agent_resolution)  # GPT-4o-mini powered\n",
    "\n",
    "    # Add fixed edges for initial processing\n",
    "    workflow.add_edge(\"classifier\", \"priority_router\")\n",
    "    workflow.add_edge(\"priority_router\", \"escalation_check\")\n",
    "\n",
    "    # Add conditional edge after escalation check - KEY DIFFERENCE!\n",
    "    workflow.add_conditional_edges(\n",
    "        \"escalation_check\",\n",
    "        route_after_escalation_check,\n",
    "        {\n",
    "            \"senior_agent\": \"senior_agent\",  # Escalated → GPT-4\n",
    "            \"normal_agent\": \"normal_agent\"   # Normal → GPT-4o-mini\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Both agents complete the resolution - no more approval gates\n",
    "    workflow.add_edge(\"senior_agent\", END)\n",
    "    workflow.add_edge(\"normal_agent\", END)\n",
    "\n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"classifier\")\n",
    "\n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "# Create the conditional workflow\n",
    "conditional_app = create_conditional_workflow()\n",
    "\n",
    "print(\"🎯 Conditional Workflow Created!\")\n",
    "print(\"\\n📊 Workflow Paths:\")\n",
    "print(\"  Path 1 (Escalated): classify → prioritize → escalation_check → senior_agent (GPT-4) → END\")\n",
    "print(\"  Path 2 (Normal): classify → prioritize → escalation_check → normal_agent (GPT-4o-mini) → END\")\n",
    "print(\"\\n💡 Key Innovation:\")\n",
    "print(\"  • Different paths use DIFFERENT AI MODELS\")\n",
    "print(\"  • Senior path gets more capable but expensive GPT-4\")\n",
    "print(\"  • Normal path gets efficient GPT-4o-mini\")\n",
    "print(\"  • No redundant approval gates - agents resolve directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yqdrnitaci",
   "metadata": {},
   "source": [
    "### Workflow Architecture:\n",
    "**Entry:** classifier \n",
    "**Initial Processing:** classifier → priority_router → escalation_check \n",
    "**Conditional Branch:** escalation_check → [senior_agent | normal_agent] \n",
    "**Exit:** [senior_agent | normal_agent] → END \n",
    "\n",
    "### Two Distinct Paths:\n",
    "**Senior Path (Escalated):**\n",
    "- Uses GPT-4 (more capable, expensive)\n",
    "- Handles complex technical issues\n",
    "- Provides comprehensive solutions\n",
    "- Direct resolution without approval\n",
    "\n",
    "**Normal Path (Standard):**\n",
    "- Uses GPT-4o-mini (efficient, cost-effective)\n",
    "- Handles routine inquiries\n",
    "- Provides quick solutions\n",
    "- Direct resolution without bureaucracy\n",
    "\n",
    "### Key Innovation:\n",
    "**Different AI Models for Different Needs**\n",
    "**Cost Optimization Through Intelligent Routing**\n",
    "**No Redundant Approval Gates**\n",
    "**Clear Path Differentiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oewwhu876vg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the conditional workflow graph with all edges\n",
    "print(\"📊 Conditional Workflow Graph Visualization:\")\n",
    "print(\"Notice the branching paths showing conditional routing!\\n\")\n",
    "\n",
    "# Display Mermaid diagram (renders automatically in Jupyter)\n",
    "from IPython.display import Markdown\n",
    "\n",
    "try:\n",
    "    mermaid_text = conditional_app.get_graph().draw_mermaid()\n",
    "    # Clean up the mermaid text - remove the config section for cleaner output\n",
    "    lines = mermaid_text.split('\\n')\n",
    "    # Skip the config section if present\n",
    "    start_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'graph TD' in line or 'graph LR' in line:\n",
    "            start_idx = i\n",
    "            break\n",
    "    clean_mermaid = '\\n'.join(lines[start_idx:])\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "### 📈 Graph with Conditional Routing (Interactive Mermaid Diagram):\n",
    "\n",
    "```mermaid\n",
    "{clean_mermaid}\n",
    "```\n",
    "\n",
    "**Notice the branching:** After the escalation_check node, the workflow splits into two paths!\n",
    "\"\"\"))\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate Mermaid diagram: {e}\")\n",
    "\n",
    "# Also show ASCII as text fallback\n",
    "print(\"\\n📈 Graph Structure with Conditional Routing (ASCII):\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    ascii_graph = conditional_app.get_graph().draw_ascii()\n",
    "    print(ascii_graph)\n",
    "except Exception as e:\n",
    "    print(f\"ASCII visualization not available: {e}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-conditional-scenarios",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different conditional scenarios showing different AI agents\n",
    "def test_conditional_scenarios():\n",
    "    \"\"\"Test scenarios to show GPT-4 vs GPT-4o-mini routing\"\"\"\n",
    "\n",
    "    print(\"🧪 Testing Conditional Workflow - Different AI Agents\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Scenario 1: Critical issue - should go to Senior Agent (GPT-4)\n",
    "    print(\"\\n📛 Scenario 1: Critical Production Outage\")\n",
    "    print(\"-\" * 50)\n",
    "    scenario1 = {\n",
    "        \"ticket_id\": \"TKT-CRITICAL\",\n",
    "        \"title\": \"URGENT: Production database down - all users affected\",\n",
    "        \"description\": \"Critical system failure, complete outage, revenue impact\",\n",
    "        \"customer_tier\": \"Premium\",\n",
    "        \"category\": \"\",\n",
    "        \"priority\": \"\",\n",
    "        \"assigned_agent\": \"\",\n",
    "        \"needs_escalation\": False,\n",
    "        \"processing_steps\": [],\n",
    "        \"status\": \"New\",\n",
    "        \"resolution\": \"\"\n",
    "    }\n",
    "\n",
    "    print(\"Expected: → Senior Agent (GPT-4) for complex resolution\")\n",
    "\n",
    "    # Scenario 2: Simple question - should go to Normal Agent (GPT-4o-mini)\n",
    "    print(\"\\n💬 Scenario 2: Basic Account Question\")\n",
    "    print(\"-\" * 50)\n",
    "    scenario2 = {\n",
    "        \"ticket_id\": \"TKT-SIMPLE\",\n",
    "        \"title\": \"How do I change my email address?\",\n",
    "        \"description\": \"I want to update my account email\",\n",
    "        \"customer_tier\": \"Standard\",\n",
    "        \"category\": \"\",\n",
    "        \"priority\": \"\",\n",
    "        \"assigned_agent\": \"\",\n",
    "        \"needs_escalation\": False,\n",
    "        \"processing_steps\": [],\n",
    "        \"status\": \"New\",\n",
    "        \"resolution\": \"\"\n",
    "    }\n",
    "\n",
    "    print(\"Expected: → Normal Agent (GPT-4o-mini) for quick resolution\")\n",
    "\n",
    "    # Test both scenarios\n",
    "    scenarios = [(\"Critical\", scenario1), (\"Simple\", scenario2)]\n",
    "\n",
    "    for scenario_name, initial_state in scenarios:\n",
    "        print(f\"\\n🔄 Executing {scenario_name} Scenario...\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        try:\n",
    "            result = conditional_app.invoke(initial_state)\n",
    "\n",
    "            print(f\"\\n✅ {scenario_name} Scenario Completed!\")\n",
    "            print(f\"  📊 Results:\")\n",
    "            print(f\"  • Category: {result['category']}\")\n",
    "            print(f\"  • Priority: {result['priority']}\")\n",
    "            print(f\"  • Escalated: {result['needs_escalation']}\")\n",
    "            print(f\"  • Assigned to: {result['assigned_agent']}\")\n",
    "\n",
    "            print(f\"\\n  📝 Resolution:\")\n",
    "            print(f\"  {result['resolution'][:200]}...\")\n",
    "\n",
    "            print(f\"\\n  🔄 Execution Path:\")\n",
    "            for i, step in enumerate(result['processing_steps'], 1):\n",
    "                print(f\"    {i}. {step}\")\n",
    "\n",
    "            # Highlight the key difference\n",
    "            if \"GPT-4)\" in result['assigned_agent']:\n",
    "                print(\"\\n  🔴 Used SENIOR AGENT with GPT-4 (more capable)\")\n",
    "            else:\n",
    "                print(\"\\n  🟢 Used NORMAL AGENT with GPT-4o-mini (efficient)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error: {str(e)}\")\n",
    "\n",
    "# Run the scenario tests\n",
    "test_conditional_scenarios()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎯 Conditional Workflow Demonstration Complete!\")\n",
    "print(\"\\n✨ Key Achievements:\")\n",
    "print(\"  • Critical issues → Senior Agent (GPT-4)\")\n",
    "print(\"  • Simple issues → Normal Agent (GPT-4o-mini)\")\n",
    "print(\"  • Different paths = Different AI capabilities\")\n",
    "print(\"  • Cost optimization through intelligent routing\")\n",
    "print(\"  • No redundant approval gates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **LangGraph's sophisticated approach** to building stateful, graph-based AI workflows with LLM intelligence at every decision point.\n",
    "\n",
    "\n",
    "## **Resources**\n",
    "\n",
    "- **Documentation**: [LangGraph Docs](https://langgraph.readthedocs.io/)\n",
    "- **GitHub**: [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph)\n",
    "- **Tutorials**: LangChain Academy LangGraph course\n",
    "- **Community**: LangChain Discord #langgraph channel\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
